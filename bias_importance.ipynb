{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from trainers.torch.networks import NetworkBody\n",
    "from mlagents_envs.base_env import ObservationSpec, DimensionProperty, ObservationType\n",
    "from mlagents.trainers.settings import NetworkSettings, TrainerSettings\n",
    "from mlagents_envs.base_env import ActionSpec\n",
    "from trainers.policy.torch_policy import TorchPolicy\n",
    "from mlagents_envs.base_env import BehaviorSpec\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_specs = [ObservationSpec(\n",
    "            name=\"position_observation\",\n",
    "            shape=(12,),  # 3D vector\n",
    "            dimension_property=(DimensionProperty.NONE,),  # Must be a tuple\n",
    "            observation_type=ObservationType.DEFAULT\n",
    "        )]\n",
    "network_settings_importance = NetworkSettings(\n",
    "    deterministic=False,\n",
    "    memory=None,\n",
    "    hidden_units=2,\n",
    "    num_layers=2,\n",
    ")\n",
    "importance_network = NetworkBody(observation_specs, network_settings_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = []\n",
    "for y in range(-9, 11, 1):\n",
    "    for x in range(-9, 11, 1):\n",
    "        positions.append([x-0.5, 0.5, y-0.5])\n",
    "positions = [torch.tensor(positions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rmarr/Projects/visibility-game-env/.visibility-game-env/lib/python3.8/site-packages/mlagents/wandb/run-20250203_005333-petjry4b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/r-marr747/bias_importance/runs/petjry4b' target=\"_blank\">graceful-mountain-1</a></strong> to <a href='https://wandb.ai/r-marr747/bias_importance' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/r-marr747/bias_importance' target=\"_blank\">https://wandb.ai/r-marr747/bias_importance</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/r-marr747/bias_importance/runs/petjry4b' target=\"_blank\">https://wandb.ai/r-marr747/bias_importance/runs/petjry4b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/r-marr747/bias_importance/runs/petjry4b?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f63b84c8250>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project='bias_importance'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000000, Loss: 2.618413077145815\n",
      "Epoch 2/100000000, Loss: 1.0066130000934936\n",
      "Epoch 3/100000000, Loss: 0.3463236474932637\n",
      "Epoch 4/100000000, Loss: 0.13324075818120037\n",
      "Epoch 5/100000000, Loss: 0.05994128439517226\n",
      "Epoch 6/100000000, Loss: 0.03005171871453058\n",
      "Epoch 7/100000000, Loss: 0.017207142108527477\n",
      "Epoch 8/100000000, Loss: 0.013849900011191494\n",
      "Epoch 9/100000000, Loss: 0.012019930292954086\n",
      "Epoch 10/100000000, Loss: 0.011454102947029697\n",
      "Epoch 11/100000000, Loss: 0.011156467818266265\n",
      "Epoch 12/100000000, Loss: 0.009397874138294\n",
      "Epoch 13/100000000, Loss: 0.009401539477360714\n",
      "Epoch 14/100000000, Loss: 0.009402829001579746\n",
      "Epoch 15/100000000, Loss: 0.008089678172404344\n",
      "Epoch 16/100000000, Loss: 0.007077647880632012\n",
      "Epoch 17/100000000, Loss: 0.006387554850293568\n",
      "Epoch 18/100000000, Loss: 0.006471185615278685\n",
      "Epoch 19/100000000, Loss: 0.0056113213393709695\n",
      "Epoch 20/100000000, Loss: 0.0059889689259936315\n",
      "Epoch 21/100000000, Loss: 0.004965538153433613\n",
      "Epoch 22/100000000, Loss: 0.005022870526251495\n",
      "Epoch 23/100000000, Loss: 0.004715611579039285\n",
      "Epoch 24/100000000, Loss: 0.004093212964802362\n",
      "Epoch 25/100000000, Loss: 0.004569747367299897\n",
      "Epoch 26/100000000, Loss: 0.0037138907248879605\n",
      "Epoch 27/100000000, Loss: 0.004121019053028618\n",
      "Epoch 28/100000000, Loss: 0.0037299123745629004\n",
      "Epoch 29/100000000, Loss: 0.003421816371902957\n",
      "Epoch 30/100000000, Loss: 0.003268015081028004\n",
      "Epoch 31/100000000, Loss: 0.0032489159665638214\n",
      "Epoch 32/100000000, Loss: 0.00293871560730775\n",
      "Epoch 33/100000000, Loss: 0.003476564719289925\n",
      "Epoch 34/100000000, Loss: 0.0028387063117074184\n",
      "Epoch 35/100000000, Loss: 0.0029827786884530426\n",
      "Epoch 36/100000000, Loss: 0.002563044258584162\n",
      "Epoch 37/100000000, Loss: 0.0027369826719748146\n",
      "Epoch 38/100000000, Loss: 0.0018785168625032077\n",
      "Epoch 39/100000000, Loss: 0.002326534283863122\n",
      "Epoch 40/100000000, Loss: 0.002327906065923688\n",
      "Epoch 41/100000000, Loss: 0.0017440241702053427\n",
      "Epoch 42/100000000, Loss: 0.002213229678615338\n",
      "Epoch 43/100000000, Loss: 0.0020435913328020663\n",
      "Epoch 44/100000000, Loss: 0.001447542148149652\n",
      "Epoch 45/100000000, Loss: 0.0017118343410238346\n",
      "Epoch 46/100000000, Loss: 0.0017746333929375968\n",
      "Epoch 47/100000000, Loss: 0.0012421068392362475\n",
      "Epoch 48/100000000, Loss: 0.0014164650916384236\n",
      "Epoch 49/100000000, Loss: 0.0014813474924819047\n",
      "Epoch 50/100000000, Loss: 0.00107666560253449\n",
      "Epoch 51/100000000, Loss: 0.0011842102057769122\n",
      "Epoch 52/100000000, Loss: 0.0010745125985939686\n",
      "Epoch 53/100000000, Loss: 0.0010305195478409579\n",
      "Epoch 54/100000000, Loss: 0.0011441306732203494\n",
      "Epoch 55/100000000, Loss: 0.001176720921530352\n",
      "Epoch 56/100000000, Loss: 0.0009388250946613952\n",
      "Epoch 57/100000000, Loss: 0.001177501095703144\n",
      "Epoch 58/100000000, Loss: 0.0010256113763381209\n",
      "Epoch 59/100000000, Loss: 0.0009088694522715124\n",
      "Epoch 60/100000000, Loss: 0.0011044084044100515\n",
      "Epoch 61/100000000, Loss: 0.0010123179470623044\n",
      "Epoch 62/100000000, Loss: 0.0007964503664593691\n",
      "Epoch 63/100000000, Loss: 0.0010153933623022037\n",
      "Epoch 64/100000000, Loss: 0.0008754224821587075\n",
      "Epoch 65/100000000, Loss: 0.0009364233021896373\n",
      "Epoch 66/100000000, Loss: 0.0009005439952922383\n",
      "Epoch 67/100000000, Loss: 0.0008721003451214995\n",
      "Epoch 68/100000000, Loss: 0.0008557383651664452\n",
      "Epoch 69/100000000, Loss: 0.0008559398936741777\n",
      "Epoch 70/100000000, Loss: 0.0008933362666211683\n",
      "Epoch 71/100000000, Loss: 0.0007718301775909353\n",
      "Epoch 72/100000000, Loss: 0.0008649653717571909\n",
      "Epoch 73/100000000, Loss: 0.0008934741126555168\n",
      "Epoch 74/100000000, Loss: 0.0008472683223597982\n",
      "Epoch 75/100000000, Loss: 0.0008173021571697836\n",
      "Epoch 76/100000000, Loss: 0.0008936260257745548\n",
      "Epoch 77/100000000, Loss: 0.0008068702325689176\n",
      "Epoch 78/100000000, Loss: 0.0009013750393622421\n",
      "Epoch 79/100000000, Loss: 0.0008136105471450916\n",
      "Epoch 80/100000000, Loss: 0.0008904432916374232\n",
      "Epoch 81/100000000, Loss: 0.0007908853708619812\n",
      "Epoch 82/100000000, Loss: 0.0007799978117346623\n",
      "Epoch 83/100000000, Loss: 0.0008638813580520815\n",
      "Epoch 84/100000000, Loss: 0.0007867658638803738\n",
      "Epoch 85/100000000, Loss: 0.0007413366797402712\n",
      "Epoch 86/100000000, Loss: 0.00086981976230922\n",
      "Epoch 87/100000000, Loss: 0.0008225569936427858\n",
      "Epoch 88/100000000, Loss: 0.0007949208003939762\n",
      "Epoch 89/100000000, Loss: 0.0007972468988252779\n",
      "Epoch 90/100000000, Loss: 0.0008204685847583883\n",
      "Epoch 91/100000000, Loss: 0.000806372928171235\n",
      "Epoch 92/100000000, Loss: 0.000804453518491725\n",
      "Epoch 93/100000000, Loss: 0.000820663816350837\n",
      "Epoch 94/100000000, Loss: 0.000809545445897238\n",
      "Epoch 95/100000000, Loss: 0.000789434494621859\n",
      "Epoch 96/100000000, Loss: 0.0008604727778432214\n",
      "Epoch 97/100000000, Loss: 0.0008061490756791833\n",
      "Epoch 98/100000000, Loss: 0.0008256320885510302\n",
      "Epoch 99/100000000, Loss: 0.00076448149999234\n",
      "Epoch 100/100000000, Loss: 0.0007776656369013362\n",
      "Epoch 101/100000000, Loss: 0.0008071477394523174\n",
      "Epoch 102/100000000, Loss: 0.0008015337306122902\n",
      "Epoch 103/100000000, Loss: 0.000803481744594432\n",
      "Epoch 104/100000000, Loss: 0.000860168159858124\n",
      "Epoch 105/100000000, Loss: 0.0007377926132697211\n",
      "Epoch 106/100000000, Loss: 0.0008264509808025958\n",
      "Epoch 107/100000000, Loss: 0.0007979044269557889\n",
      "Epoch 108/100000000, Loss: 0.0007102885146013241\n",
      "Epoch 109/100000000, Loss: 0.0008410954443823776\n",
      "Epoch 110/100000000, Loss: 0.0008174870902855606\n",
      "Epoch 111/100000000, Loss: 0.0006844983006210348\n",
      "Epoch 112/100000000, Loss: 0.0008038652543897916\n",
      "Epoch 113/100000000, Loss: 0.0008471629901070118\n",
      "Epoch 114/100000000, Loss: 0.0007725460259244271\n",
      "Epoch 115/100000000, Loss: 0.000766169782870802\n",
      "Epoch 116/100000000, Loss: 0.0009106213048339666\n",
      "Epoch 117/100000000, Loss: 0.0007414097888919407\n",
      "Epoch 118/100000000, Loss: 0.0007416880295134753\n",
      "Epoch 119/100000000, Loss: 0.000865483905128932\n",
      "Epoch 120/100000000, Loss: 0.0007130551449830641\n",
      "Epoch 121/100000000, Loss: 0.0007947529245649354\n",
      "Epoch 122/100000000, Loss: 0.0008036953523959455\n",
      "Epoch 123/100000000, Loss: 0.0007493612160850676\n",
      "Epoch 124/100000000, Loss: 0.0007830522370463825\n",
      "Epoch 125/100000000, Loss: 0.0008274611343453839\n",
      "Epoch 126/100000000, Loss: 0.0007796901585770441\n",
      "Epoch 127/100000000, Loss: 0.0008038671540049202\n",
      "Epoch 128/100000000, Loss: 0.0006899263291988947\n",
      "Epoch 129/100000000, Loss: 0.0008455952288989163\n",
      "Epoch 130/100000000, Loss: 0.0007602136001490179\n",
      "Epoch 131/100000000, Loss: 0.0007428116513221402\n",
      "Epoch 132/100000000, Loss: 0.0007645421039587646\n",
      "Epoch 133/100000000, Loss: 0.0007723725857804309\n",
      "Epoch 134/100000000, Loss: 0.00070283095980194\n",
      "Epoch 135/100000000, Loss: 0.000823598629394007\n",
      "Epoch 136/100000000, Loss: 0.0007872300283116471\n",
      "Epoch 137/100000000, Loss: 0.0007164408871833388\n",
      "Epoch 138/100000000, Loss: 0.0008064458873410435\n",
      "Epoch 139/100000000, Loss: 0.000764927026286188\n",
      "Epoch 140/100000000, Loss: 0.0007046465071283776\n",
      "Epoch 141/100000000, Loss: 0.0007707328923410866\n",
      "Epoch 142/100000000, Loss: 0.0007729743091050423\n",
      "Epoch 143/100000000, Loss: 0.0007012954352574181\n",
      "Epoch 144/100000000, Loss: 0.0008248915748743757\n",
      "Epoch 145/100000000, Loss: 0.0007458880181959845\n",
      "Epoch 146/100000000, Loss: 0.0007984045072339325\n",
      "Epoch 147/100000000, Loss: 0.0007559645977785928\n",
      "Epoch 148/100000000, Loss: 0.0007547384224429652\n",
      "Epoch 149/100000000, Loss: 0.0007306103090586802\n",
      "Epoch 150/100000000, Loss: 0.0007964674549623551\n",
      "Epoch 151/100000000, Loss: 0.0007397388178364566\n",
      "Epoch 152/100000000, Loss: 0.0007602144470252041\n",
      "Epoch 153/100000000, Loss: 0.0007410930048292034\n",
      "Epoch 154/100000000, Loss: 0.0007741594263162863\n",
      "Epoch 155/100000000, Loss: 0.0007438767371157601\n",
      "Epoch 156/100000000, Loss: 0.000766030241364027\n",
      "Epoch 157/100000000, Loss: 0.0007366111481810395\n",
      "Epoch 158/100000000, Loss: 0.000715413491472341\n",
      "Epoch 159/100000000, Loss: 0.0007583844157786499\n",
      "Epoch 160/100000000, Loss: 0.0007692246922866356\n",
      "Epoch 161/100000000, Loss: 0.0007012254006580564\n",
      "Epoch 162/100000000, Loss: 0.000823845700887216\n",
      "Epoch 163/100000000, Loss: 0.000720887109453751\n",
      "Epoch 164/100000000, Loss: 0.0006761397796808964\n",
      "Epoch 165/100000000, Loss: 0.0008395619430132517\n",
      "Epoch 166/100000000, Loss: 0.0006885172175058558\n",
      "Epoch 167/100000000, Loss: 0.0007174787824878315\n",
      "Epoch 168/100000000, Loss: 0.0008267520905554083\n",
      "Epoch 169/100000000, Loss: 0.0007285162627257938\n",
      "Epoch 170/100000000, Loss: 0.000707409787756499\n",
      "Epoch 171/100000000, Loss: 0.0007932742871032905\n",
      "Epoch 172/100000000, Loss: 0.0007212719048283401\n",
      "Epoch 173/100000000, Loss: 0.0007273561029232502\n",
      "Epoch 174/100000000, Loss: 0.0007453495753347272\n",
      "Epoch 175/100000000, Loss: 0.0007274861339139363\n",
      "Epoch 176/100000000, Loss: 0.0007296774599966223\n",
      "Epoch 177/100000000, Loss: 0.0007291977225789227\n",
      "Epoch 178/100000000, Loss: 0.0007630073667560774\n",
      "Epoch 179/100000000, Loss: 0.0007067443671242177\n",
      "Epoch 180/100000000, Loss: 0.0007107945353539069\n",
      "Epoch 181/100000000, Loss: 0.0007578477410145836\n",
      "Epoch 182/100000000, Loss: 0.0007420918385262625\n",
      "Epoch 183/100000000, Loss: 0.00071485382138394\n",
      "Epoch 184/100000000, Loss: 0.0007234970010857755\n",
      "Epoch 185/100000000, Loss: 0.000751814386268434\n",
      "Epoch 186/100000000, Loss: 0.0006960292593182082\n",
      "Epoch 187/100000000, Loss: 0.000773364659296254\n",
      "Epoch 188/100000000, Loss: 0.0007317648520545784\n",
      "Epoch 189/100000000, Loss: 0.0006861574546266856\n",
      "Epoch 190/100000000, Loss: 0.0007318915306193485\n",
      "Epoch 191/100000000, Loss: 0.0007528490431414591\n",
      "Epoch 192/100000000, Loss: 0.0006710943582853837\n",
      "Epoch 193/100000000, Loss: 0.0007389891504739397\n",
      "Epoch 194/100000000, Loss: 0.0007481894599350383\n",
      "Epoch 195/100000000, Loss: 0.0006908636022240782\n",
      "Epoch 196/100000000, Loss: 0.0007220332076739542\n",
      "Epoch 197/100000000, Loss: 0.0007312213420532371\n",
      "Epoch 198/100000000, Loss: 0.0007030064871422216\n",
      "Epoch 199/100000000, Loss: 0.0007191059531884181\n",
      "Epoch 200/100000000, Loss: 0.0007415239029520226\n",
      "Epoch 201/100000000, Loss: 0.0007003911975688598\n",
      "Epoch 202/100000000, Loss: 0.0007217057047562361\n",
      "Epoch 203/100000000, Loss: 0.0007437101069656715\n",
      "Epoch 204/100000000, Loss: 0.0006988506942990109\n",
      "Epoch 205/100000000, Loss: 0.0006872268908981074\n"
     ]
    }
   ],
   "source": [
    "target_tensor = torch.tensor([0.0, 1.0])\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(importance_network.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Create positions\n",
    "positions = []\n",
    "for y in range(-9, 11, 1):\n",
    "    for x in range(-9, 11, 1):\n",
    "        positions.append([x - 0.5, 0.5, y - 0.5])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100000000  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for pos in positions:\n",
    "        # Add random values from [0, 1] of size 9\n",
    "        random_values = np.random.rand(9).tolist()\n",
    "        input_data = pos + random_values\n",
    "\n",
    "        # Convert to tensor and wrap in a list\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32).unsqueeze(0)\n",
    "        input_tensor_list = [input_tensor]  # Wrap in a list if the network expects a list\n",
    "\n",
    "        # Forward pass through the network\n",
    "        output = importance_network(input_tensor_list)[0].squeeze(0)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = mse_loss(output, target_tensor)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = epoch_loss / len(positions)\n",
    "\n",
    "    # Log the average loss to wandb\n",
    "    wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}')\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/rmarr/Projects/visibility-game-env/informed_init\"\n",
    "checkpoint_path = os.path.join(model_path, f\"{'bias_importance'}.pt\")\n",
    "state_dict = importance_network.state_dict()\n",
    "torch.save(state_dict, checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".visibility-game-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
